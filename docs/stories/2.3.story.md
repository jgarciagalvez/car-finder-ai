# <!-- Powered by BMAD™ Core -->

# Story 2.3: AI Analysis Features

## Status
In Progress (Core implementation complete, tests pending)

## Story
**As a** user,
**I want** comprehensive AI analysis for each vehicle including fit scores and reports,
**so that** I have intelligent insights to guide my vehicle selection decisions.

## Acceptance Criteria
1. Vehicle descriptions and equipment lists are translated from Polish to English and stored in the `description` and `features` fields.
2. Personal Fit Score generation is implemented using LLM analysis of vehicle data against user criteria.
3. AI Priority Rating and natural-language summaries are generated synthesizing all data points.
4. Virtual Mechanic's Report is created providing model-specific mechanical insights and inspection points.
5. Data Sanity Check is implemented to flag inconsistencies between structured data and descriptions.
6. AI prompts are organized as versioned markdown files following BMAD-style agent patterns, not hardcoded in TypeScript.
7. A separate script (`apps/api/src/scripts/analyze.ts`) is created to run analysis on all un-analyzed vehicles.

## Tasks / Subtasks
- [ ] Design prompt file structure and organization (AC: 6)
  - [ ] Create `packages/ai/src/prompts/` directory
  - [ ] Design markdown prompt template format (Agent Role, Task, Input Schema, Instructions, Output Format, Examples)
  - [ ] Create PromptLoader utility in `packages/ai/src/utils/PromptLoader.ts`
  - [ ] Implement markdown parsing for extracting prompt sections
  - [ ] Implement variable interpolation for dynamic prompt building
  - [ ] Add prompt caching for performance
  - [ ] Add prompt validation on load (ensure required sections present)
  - [ ] Export PromptLoader from packages/ai
- [ ] Create prompt definition files (AC: 1, 2, 3, 4, 5, 6)
  - [ ] Create `translate-vehicle.md` for translating description and equipment in one call
  - [ ] Create `personal-fit-score.md` with agent role, scoring rubric, and examples
  - [ ] Create `priority-rating.md` with synthesis instructions and output format
  - [ ] Create `mechanic-report.md` with inspection point guidelines and model-specific analysis
  - [ ] Create `sanity-check.md` with consistency checking rules and flag definitions
  - [ ] Include input/output schemas in each prompt file
  - [ ] Include at least one example per prompt file
  - [ ] Document prompt versioning strategy in prompts/README.md
- [ ] Create AIService in backend API package (AC: 1, 2, 3, 4, 5, 6)
  - [ ] Create `apps/api/src/services/AIService.ts` file
  - [ ] Import AIProviderFactory and PromptLoader from `packages/ai`
  - [ ] Define UserCriteria interface for Personal Fit Score analysis
  - [ ] Implement `translateVehicleContent(vehicle)` using PromptLoader (returns description + features)
  - [ ] Implement `generatePersonalFitScore(vehicle, criteria)` using PromptLoader
  - [ ] Implement `generatePriorityRating(vehicle)` using PromptLoader
  - [ ] Implement `generateMechanicReport(vehicle)` using PromptLoader
  - [ ] Implement `generateDataSanityCheck(vehicle)` using PromptLoader
  - [ ] Add comprehensive error handling for AI operations and prompt loading
  - [ ] Add logging for which prompt versions are being used
  - [ ] Export AIService for use in analysis script
- [ ] Implement vehicle content translation (AC: 1, 6)
  - [ ] Load translate-vehicle.md prompt using PromptLoader
  - [ ] Build prompt with sourceDescriptionHtml and sourceEquipment
  - [ ] Extract translated description (plain text, English) from LLM response
  - [ ] Extract translated and normalized features array from LLM response
  - [ ] Validate translation output format and handle errors
  - [ ] Add unit tests for translation logic with mocked prompts
- [ ] Implement Personal Fit Score generation (AC: 2, 6)
  - [ ] Load personal-fit-score.md prompt using PromptLoader
  - [ ] Build prompt with vehicle data and user criteria
  - [ ] Extract structured score (0-10) from LLM response
  - [ ] Parse reasoning, strengths, and concerns from response
  - [ ] Validate LLM response format and handle errors
  - [ ] Add unit tests for score generation logic with mocked prompts
- [ ] Implement AI Priority Rating and Summary (AC: 3, 6)
  - [ ] Load priority-rating.md prompt using PromptLoader
  - [ ] Build prompt synthesizing all vehicle data points
  - [ ] Extract priority rating (0-10) from LLM response
  - [ ] Extract natural-language summary from LLM response
  - [ ] Validate response format and handle parsing errors
  - [ ] Add unit tests for priority rating logic
- [ ] Implement Virtual Mechanic's Report (AC: 4, 6)
  - [ ] Load mechanic-report.md prompt using PromptLoader
  - [ ] Build prompt with vehicle make, model, year, mileage
  - [ ] Extract inspection points and red flags from response
  - [ ] Format response as structured markdown report
  - [ ] Validate response completeness
  - [ ] Add unit tests for mechanic report generation
- [ ] Implement Data Sanity Check (AC: 5, 6)
  - [ ] Load sanity-check.md prompt using PromptLoader
  - [ ] Build prompt with sourceParameters and sourceDescriptionHtml
  - [ ] Parse flags and inconsistency warnings from response
  - [ ] Return structured sanity check result
  - [ ] Add unit tests for sanity check logic
- [ ] Create analysis script (AC: 7)
  - [ ] Create `apps/api/src/scripts/analyze.ts` file
  - [ ] Import AIService and VehicleRepository
  - [ ] Implement query to find vehicles with NULL translation or AI fields
  - [ ] Implement batch processing loop with rate limit respect (15 RPM)
  - [ ] Call translateVehicleContent() FIRST for each vehicle (if description is NULL)
  - [ ] Call remaining AIService methods for analysis (Personal Fit, Priority, Mechanic, Sanity Check)
  - [ ] Update vehicle records in database with translation and AI results
  - [ ] Add progress logging and error handling
  - [ ] Generate summary report of analysis completion
  - [ ] Add CLI arguments for filtering (specific vehicle ID, source, etc.)
  - [ ] Add CLI flag to skip specific analyses (--skip-translation, --skip-mechanic-report, etc.)
- [ ] Add comprehensive error handling and validation
  - [ ] Handle Gemini API errors gracefully
  - [ ] Implement retry logic for transient failures (use RetryHandler from packages/ai)
  - [ ] Validate AI responses before saving to database
  - [ ] Log failed analyses for debugging
  - [ ] Continue processing remaining vehicles if one fails
- [ ] Create comprehensive test suite
  - [ ] Write unit tests for AIService with mocked AI provider
  - [ ] Test vehicle content translation (description + features)
  - [ ] Test Personal Fit Score generation with sample data
  - [ ] Test Priority Rating with various vehicle profiles
  - [ ] Test Mechanic Report generation for different models
  - [ ] Test Data Sanity Check with inconsistent data
  - [ ] Test error handling for malformed AI responses
  - [ ] Create integration test for analyze.ts script with test database
  - [ ] Mock Gemini API calls in tests to avoid rate limits

## Dev Notes

### Previous Story Insights
From Story 2.2 (AI Infrastructure), the following is now available:
- **AI Provider Abstraction Layer**: Complete `packages/ai` package with IAIProvider interface and AIProviderFactory
- **Gemini Provider**: GeminiProvider class with authentication, rate limiting, and retry logic
- **Prompt Engineering Utilities**: PromptBuilder utility for structured prompt construction
- **Response Validation**: ResponseValidator for validating AI responses
- **Rate Limiting**: RateLimiter utility respecting 60 requests per minute (Gemini free tier)
- **Error Handling**: Comprehensive error types (AIError, RateLimitError, ValidationError)

**Key Pattern to Follow**: Use the factory pattern to instantiate the AI provider, similar to how other services use the abstraction layer. Do NOT directly instantiate GeminiProvider.

### Architecture Context

**IMPORTANT ARCHITECTURAL NOTE** (Updated Oct 9, 2025):
The background processing scripts have been relocated from `packages/scripts` to `apps/api/src/scripts` to resolve architectural violations and improve maintainability:

**Rationale for Change:**
- **Eliminates Fragile Imports**: Scripts were using `../../../apps/api/src/services/` imports, which broke when running from different working directories
- **Proper Package Boundaries**: Scripts are execution contexts (like HTTP routes), not shared libraries
- **Unified Backend**: All backend code (services, routes, scripts) now lives cohesively in `apps/api`
- **Clean Local Imports**: Scripts now use `../services/AIService` instead of complex relative paths
- **Package Clarity**: Only truly shared libraries remain in `packages/` (types, db, ai, services)

**CLI Command Changes:**
- **Old**: `pnpm --filter @car-finder/scripts analyze`
- **New**: `pnpm analyze` (delegates to `@car-finder/api`)

**Import Pattern Changes:**
- **Old**: `import { AIService } from '../../../apps/api/src/services/AIService'`
- **New**: `import { AIService } from '../services/AIService'`

All references to script locations in this story have been updated to reflect the new structure.

---

**AI Service Location** [Source: architecture/source-tree.md]:
```
apps/api/src/services/AIService.ts    # New backend service for AI operations
apps/api/src/scripts/analyze.ts       # New script for batch AI analysis
```

**Service Architecture Pattern** [Source: architecture/backend-architecture.md]:
- **Service Layer**: Business logic in reusable services shared between API and scripts
- **Repository Pattern**: All database operations through `packages/db` repository layer
- **Separation of Concerns**: Each service has single responsibility, minimal dependencies

**AI Provider Usage** [Source: architecture/coding-standards.md]:
- **Critical Rule**: AI operations must use the provider abstraction layer from `packages/ai`
- **Types**: Use shared types from `packages/types`
- **Error Handling**: Use error types from `packages/ai/src/interfaces/errors.ts`

### Data Models

**Vehicle Interface** [Source: architecture/data-models.md]:
The AI analysis will populate these fields in the Vehicle model:
```typescript
interface Vehicle {
  // ... existing fields ...

  // Processed & Normalized Data (to be populated by translation)
  description: string;                    // Translated plain-text description (English)
  features: string[];                     // Normalized feature list (English, e.g., ["comfort_air_conditioning"])

  // AI Generated Data (currently NULL, to be populated)
  personalFitScore: number | null;        // 0-10 score based on user criteria
  marketValueScore: string | null;        // e.g., "-5%" or "+10%" (calculated by MarketValueService in future story)
  aiPriorityRating: number | null;        // 0-10 overall priority score
  aiPrioritySummary: string | null;       // Natural language summary
  aiMechanicReport: string | null;        // Markdown-formatted report
  aiDataSanityCheck: string | null;       // Inconsistency flags and warnings
}
```

**UserCriteria Interface** (to be defined in AIService):
```typescript
interface UserCriteria {
  budgetEur: { min: number; max: number };
  preferredFeatures: string[];      // e.g., ["air_conditioning", "leather_seats"]
  useCase: string;                  // e.g., "daily commute", "family car"
  priorityFactors: string[];        // e.g., ["fuel_efficiency", "reliability"]
}
```

### Prompt Organization Architecture

**BMAD-Style Prompt Structure** [Inspired by BMAD Core patterns]:

This story introduces a declarative, markdown-based prompt organization system similar to how BMAD defines agent personas and workflows. Prompts are versioned artifacts, not hardcoded strings.

**Benefits**:
- **Iterate without recompiling**: Tune prompts independently of code
- **Version control**: Git tracks prompt evolution separately
- **Testable**: Validate prompt files independently
- **Collaborative**: Non-engineers can propose improvements
- **Maintainable**: Clear separation of concerns (logic vs. prompts)

**Prompt File Structure**:
```
packages/ai/src/prompts/
├── README.md                    # Prompt versioning strategy
├── translate-vehicle.md         # Translation agent (description + equipment → description + features)
├── personal-fit-score.md        # Personal Fit Score analyzer agent
├── priority-rating.md           # Priority Rating synthesizer agent
├── mechanic-report.md           # Virtual Mechanic agent
└── sanity-check.md              # Data consistency checker agent
```

**Markdown Prompt Template Format**:
```markdown
<!-- AI Prompt Definition: [Prompt Name] -->

# [Prompt Name]

## Agent Role
[Define the AI's persona and expertise]

## Task
[Clear description of what the AI needs to accomplish]

## Input Schema
```json
{
  "field": "type and description"
}
```

## Instructions
1. [Step-by-step analysis instructions]
2. [Specific considerations]
3. [Quality criteria]

## Scoring Rubric (if applicable)
- **9-10**: [Excellent criteria]
- **7-8**: [Good criteria]
- ...

## Output Format
```json
{
  "field": "expected output structure"
}
```

## Example
### Input
[Example input data]

### Output
[Expected output for example]
```

**PromptLoader Utility**:
```typescript
// packages/ai/src/utils/PromptLoader.ts
export interface ParsedPrompt {
  role: string;
  task: string;
  instructions: string[];
  inputSchema: object;
  outputFormat: object;
  examples?: Array<{ input: any; output: any }>;
}

export class PromptLoader {
  static async loadPrompt(promptName: string): Promise<ParsedPrompt>;
  static buildPrompt(parsed: ParsedPrompt, variables: Record<string, any>): string;
  private static parsePromptMarkdown(content: string): ParsedPrompt;
}
```

### AI Provider Integration

**Using AIProviderFactory and PromptLoader** [Source: Story 2.2 Dev Notes]:
```typescript
import { AIProviderFactory, PromptLoader } from '@car-finder/ai';

class AIService {
  private provider: IAIProvider;

  constructor(apiKey: string) {
    this.provider = AIProviderFactory.createProvider({
      provider: 'gemini',
      apiKey,
      rateLimitConfig: {
        requestsPerMinute: 60,
        retryAttempts: 3,
        retryDelayMs: 1000,
      },
    });
  }

  async generatePersonalFitScore(vehicle: Vehicle, criteria: UserCriteria): Promise<number> {
    // Load prompt definition from markdown file
    const prompt = await PromptLoader.loadPrompt('personal-fit-score');

    // Build prompt with vehicle data interpolated
    const fullPrompt = PromptLoader.buildPrompt(prompt, {
      vehicle: this.serializeVehicle(vehicle),
      criteria: this.serializeCriteria(criteria),
    });

    // Call AI provider with structured output
    const response = await this.provider.generateStructured(
      fullPrompt,
      prompt.outputFormat
    );

    return response.score;
  }
}
```

**IAIProvider Interface** [Source: Story 2.2 Dev Notes]:
```typescript
interface IAIProvider {
  generateText(prompt: string, options?: GenerationOptions): Promise<string>;
  chat(messages: ChatMessage[], options?: GenerationOptions): Promise<string>;
  generateStructured<T>(prompt: string, schema: object, options?: GenerationOptions): Promise<T>;
}
```

### Functional Requirements Reference

**FR5: Personal Fit Score** [Source: prd/requirements.md]:
- Use LLM to generate score based on predefined user criteria
- Score should be 0-10 numeric value
- Include reasoning/explanation for the score

**FR7: AI Priority Rating** [Source: prd/requirements.md]:
- Synthesize ALL data points (price, scores, condition, features)
- Generate 0-10 priority rating
- Create natural-language summary explaining the rating

**FR8: Virtual Mechanic's Report** [Source: prd/requirements.md]:
- Model-specific mechanical insights
- List inspection points relevant to the vehicle
- Flag red flags and common issues for the model/year

**FR9: Data Sanity Check** [Source: prd/requirements.md]:
- Compare structured data (sourceParameters) against text description (sourceDescriptionHtml)
- Flag inconsistencies (e.g., mileage mismatch, contradictory features)
- Return warnings as structured text

**NFR2: Cost Optimization** [Source: prd/requirements.md]:
- Stay within Gemini API free tier (15 RPM, 1500 RPD)
- No need to re-analyze vehicles (check for NULL fields)
- Cache results in database

**NFR6: Fault Tolerance** [Source: prd/requirements.md]:
- Save AI responses immediately after generation
- Allow resuming from failures (skip already-analyzed vehicles)
- Log errors for debugging

### File Locations and Structure

**Prompt Files Structure** (NEW):
```
packages/ai/src/prompts/
├── README.md                          # Prompt versioning and maintenance guide
├── translate-vehicle.md               # Translation agent (description + equipment → description + features)
├── personal-fit-score.md              # Personal Fit Score analyzer agent
├── priority-rating.md                 # Priority Rating synthesizer agent
├── mechanic-report.md                 # Virtual Mechanic agent
└── sanity-check.md                    # Data consistency checker agent
```

**PromptLoader Utility** (NEW):
```
packages/ai/src/utils/
├── PromptLoader.ts                    # Markdown prompt parser and loader
│   ├── interface ParsedPrompt         # Structured prompt representation
│   ├── loadPrompt(name: string)       # Load and parse markdown prompt
│   ├── buildPrompt(parsed, vars)      # Interpolate variables into prompt
│   └── parsePromptMarkdown(content)   # Parse markdown sections
└── index.ts                           # Export PromptLoader
```

**AIService Implementation** [Source: architecture/source-tree.md + Prompt Infrastructure]:
```
apps/api/src/services/AIService.ts
├── Imports: AIProviderFactory, PromptLoader from @car-finder/ai
├── Imports: Vehicle from @car-finder/types
├── Class: AIService
│   ├── private provider: IAIProvider
│   ├── constructor(apiKey: string)
│   ├── translateVehicleContent(vehicle): Promise<{ description: string; features: string[] }>
│   │   └── Uses PromptLoader.loadPrompt('translate-vehicle')
│   ├── generatePersonalFitScore(vehicle, criteria): Promise<number>
│   │   └── Uses PromptLoader.loadPrompt('personal-fit-score')
│   ├── generatePriorityRating(vehicle): Promise<{ rating: number; summary: string }>
│   │   └── Uses PromptLoader.loadPrompt('priority-rating')
│   ├── generateMechanicReport(vehicle): Promise<string>
│   │   └── Uses PromptLoader.loadPrompt('mechanic-report')
│   └── generateDataSanityCheck(vehicle): Promise<string>
│       └── Uses PromptLoader.loadPrompt('sanity-check')
```

**Analysis Pipeline Order**:
The analyze.ts script must execute AI operations in this specific order:
1. **Translation** (description + features) - Must run FIRST to provide English content for other analyses
2. **Data Sanity Check** - Detect data issues early before other analyses
3. **Personal Fit Score** - Uses translated description and criteria
4. **Virtual Mechanic's Report** - Model-specific mechanical insights
5. **Priority Rating** - Synthesizes ALL data, must run LAST

**Analysis Script** [Source: architecture/source-tree.md]:
```
apps/api/src/scripts/analyze.ts
├── Imports: AIService from ../services/AIService
├── Imports: VehicleRepository from @car-finder/db
├── Function: main()
│   ├── Initialize AIService with GEMINI_API_KEY
│   ├── Query vehicles with NULL AI fields
│   ├── Loop through vehicles (with rate limiting)
│   ├── Generate and save AI analysis for each vehicle
│   └── Print summary report
```

### Environment Configuration

**Required Environment Variables** [Source: Story 2.2 Dev Notes]:
```bash
# Already configured in root .env from Story 2.2
GEMINI_API_KEY=your_api_key_here
AI_PROVIDER=gemini
AI_MODEL=gemini-2.5-pro
AI_RATE_LIMIT_RPM=60
AI_MAX_RETRIES=3
AI_RETRY_DELAY_MS=1000
```

No new environment variables are needed for this story.

### Rate Limiting Strategy

**Gemini API Free Tier Limits** [Source: architecture/external-apis.md]:
- 15 requests per minute (RPM)
- 1,500 requests per day (RPD)
- 1M tokens per minute (TPM)

**Implementation Strategy**:
- Use RateLimiter from `packages/ai` (already configured for 60 RPM, will respect actual 15 RPM limit)
- Process vehicles sequentially in analyze.ts script
- Add 4-second delay between vehicles (15 vehicles per minute = 4s each)
- RetryHandler will handle 429 rate limit errors automatically
- Log progress to show which vehicles are being analyzed

### Database Operations

**Query for Un-Analyzed Vehicles** [Source: architecture/database-schema.md]:
```typescript
// In VehicleRepository, add method:
async findVehiclesWithoutAnalysis(): Promise<Vehicle[]> {
  return await db
    .selectFrom('vehicles')
    .selectAll()
    .where('description', 'is', null)  // Translation not done yet
    .orWhere('personalFitScore', 'is', null)
    .orWhere('aiPriorityRating', 'is', null)
    .orWhere('aiMechanicReport', 'is', null)
    .orWhere('aiDataSanityCheck', 'is', null)
    .execute();
}
```

**Update Vehicle with AI Analysis**:
```typescript
// In VehicleRepository, add method:
async updateVehicleAnalysis(id: string, analysis: {
  description?: string;           // Translated description
  features?: string[];            // Translated and normalized features
  personalFitScore?: number;
  aiPriorityRating?: number;
  aiPrioritySummary?: string;
  aiMechanicReport?: string;
  aiDataSanityCheck?: string;
}): Promise<void> {
  await db
    .updateTable('vehicles')
    .set(analysis)
    .where('id', '=', id)
    .execute();
}
```

### Error Handling Requirements

**Error Types from packages/ai** [Source: Story 2.2 Dev Notes]:
```typescript
import { AIError, RateLimitError, ValidationError } from '@car-finder/ai';
```

**Error Handling Strategy**:
1. Catch AIError, RateLimitError, ValidationError from AI provider calls
2. Log errors with vehicle ID and error details
3. Continue processing remaining vehicles (don't fail entire batch)
4. Return partial results if some analyses succeed
5. Use RetryHandler for transient failures (network errors, 429s)
6. Don't retry on permanent failures (400, 401 errors)

### Technical Constraints

**TypeScript Requirements** [Source: architecture/tech-stack.md]:
- TypeScript ~5.5.0
- Explicit return types for all functions
- Strict null checks enabled
- No `any` types (use `unknown` if needed)

**Node.js Version** [Source: architecture/tech-stack.md]:
- Node.js ~20.11.0 (LTS)

**Dependencies**:
```json
{
  "dependencies": {
    "@car-finder/ai": "workspace:*",
    "@car-finder/types": "workspace:*",
    "@car-finder/db": "workspace:*"
  }
}
```

### Naming Conventions

**Files** [Source: architecture/coding-standards.md]:
- Services: `camelCase` - `AIService.ts`
- Scripts: `camelCase` - `analyze.ts`

**Functions** [Source: architecture/coding-standards.md]:
- Functions: `camelCase` - `generatePersonalFitScore()`, `generatePriorityRating()`
- Private methods: `_camelCase` - `_buildPrompt()`

**Types** [Source: architecture/coding-standards.md]:
- Interfaces: `PascalCase` with `I` prefix - `IAIService`, `IUserCriteria`
- Types: `PascalCase` - `AnalysisResult`, `FitScoreResponse`

### Integration with Service Abstraction Layer

This story creates a concrete service (AIService) that will later be abstracted into `packages/services` if needed. For now, it can live in `apps/api/src/services/` as it's only used by the analysis script.

**Future Integration** (not part of this story):
- Could create `IAIService` interface in `packages/services`
- Could create `MockAIService` for testing
- Could register with ServiceRegistry for dependency injection

## Testing

### Testing Standards

**Testing Framework** [Source: architecture/testing-strategy.md]:
- Jest ~29.7.0 for unit and integration tests
- Co-located `*.test.ts` files next to source files
- Focus on critical paths and complex logic

**Test Organization** [Source: architecture/testing-strategy.md]:
- Backend tests in `apps/api/__tests__/` and `packages/scripts/src/__tests__/`
- Mock external dependencies (Gemini API) to avoid rate limits
- Use in-memory LibSQL for database tests

### Testing Requirements for This Story

**PromptLoader Unit Tests** (`packages/ai/src/utils/PromptLoader.test.ts`):
- Test loading valid prompt markdown files
- Test parsing markdown sections (Agent Role, Task, Instructions, Output Format)
- Test extracting JSON schemas from code blocks
- Test variable interpolation in buildPrompt()
- Test prompt caching mechanism
- Test error handling for missing prompt files
- Test error handling for malformed markdown
- Test validation of required sections

**AIService Unit Tests** (`apps/api/src/services/AIService.test.ts`):
- Test vehicle content translation with Polish HTML input
- Test translation output format (description string + features array)
- Test Personal Fit Score generation with sample vehicle and criteria
- Test Priority Rating generation with various vehicle profiles
- Test Mechanic Report generation for different vehicle models
- Test Data Sanity Check with intentionally inconsistent data
- Mock AI provider responses to avoid real API calls
- Mock PromptLoader to avoid file I/O in tests
- Test error handling for malformed AI responses
- Test error handling for API failures (network, rate limits)
- Test error handling for prompt loading failures
- Verify proper use of AIProviderFactory for provider instantiation

**Analysis Script Integration Tests** (`apps/api/src/scripts/__tests__/analyze.test.ts`):
- Test script identifies vehicles with NULL translation or AI fields correctly
- Test script runs translation FIRST before other analyses
- Test script processes vehicles in batches respecting rate limits
- Test script updates database with translation and AI analysis results
- Test script handles errors and continues processing remaining vehicles
- Test script generates accurate summary report
- Test CLI flags (--skip-translation, --skip-mechanic-report, etc.)
- Use in-memory test database with sample vehicles
- Mock AIService to avoid real AI calls

**Repository Tests** (`packages/db/src/__tests__/VehicleRepository.test.ts`):
- Test `findVehiclesWithoutAnalysis()` query returns vehicles with NULL description or AI fields
- Test `updateVehicleAnalysis()` updates translation and AI fields correctly
- Test partial updates (some AI fields populated, others NULL)
- Use in-memory LibSQL database

**Prompt File Validation** (optional, manual validation):
- Manually test translation prompt with real Polish vehicle data
- Manually test analysis prompts with real Gemini API to verify output quality
- Validate prompt responses match expected format
- Iterate on prompt wording if LLM responses are inconsistent
- Ensure all 5 prompt files follow the standard template format

### Test Coverage Goals

- Aim for >80% code coverage for AIService methods
- 100% coverage for error handling paths
- Test all edge cases (NULL data, missing fields, malformed responses)
- Mock external dependencies in unit tests

### Mock Implementation

**Mocking AI Provider in Tests**:
```typescript
const mockProvider: IAIProvider = {
  generateText: jest.fn().mockResolvedValue('{"score": 8, "reasoning": "Good fit"}'),
  chat: jest.fn(),
  generateStructured: jest.fn().mockResolvedValue({ score: 8, reasoning: "Good fit" }),
  getProviderName: jest.fn().mockReturnValue('mock'),
  getModelInfo: jest.fn().mockReturnValue({ name: 'mock-model', version: '1.0' }),
};
```

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|---------|
| Oct 8, 2025 | 1.0 | Initial story creation for Epic 2, Story 2.3 | Bob (SM) |
| Oct 8, 2025 | 1.1 | Added BMAD-style prompt organization with markdown files and PromptLoader utility (AC5 added) | John (PM) |
| Oct 9, 2025 | 1.2 | Updated script locations from `packages/scripts` to `apps/api/src/scripts` per architectural refactor - improved import patterns and package boundaries | John (PM) |
| Oct 9, 2025 | 1.3 | Added vehicle content translation (description + features) as AC1, new prompt translate-vehicle.md, translateVehicleContent() method in AIService - translation runs FIRST in analysis pipeline to provide English content for other analyses | John (PM) |

## Dev Agent Record

### Agent Model Used
*To be populated by Dev Agent*

### Debug Log References
*To be populated by Dev Agent*

### Completion Notes List
*To be populated by Dev Agent*

### File List
*To be populated by Dev Agent*

## QA Results
*Results from QA Agent review will be populated here after story completion*
